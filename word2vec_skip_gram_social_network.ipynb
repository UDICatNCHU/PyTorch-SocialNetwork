{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "word2vec skip-gram@social network.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/UDICatNCHU/PyTorch-SocialNetwork/blob/master/word2vec_skip_gram_social_network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aP45bTGBO-oB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "import torch.functional as F\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ps2XL34PHKD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus = [\n",
        "    'he is a king',\n",
        "    'she is a queen',\n",
        "    'he is a man',\n",
        "    'she is a woman',\n",
        "    'warsaw is poland capital',\n",
        "    'berlin is germany capital',\n",
        "    'paris is france capital',   \n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JuHa7LMJPMQp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize_corpus(corpus):\n",
        "    tokens = [x.split() for x in corpus]\n",
        "    return tokens\n",
        "\n",
        "tokenized_corpus = tokenize_corpus(corpus)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SUR0-Mp9PQ3c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocabulary = []\n",
        "for sentence in tokenized_corpus:\n",
        "    for token in sentence:\n",
        "        if token not in vocabulary:\n",
        "            vocabulary.append(token)\n",
        "\n",
        "word2idx = {w: idx for (idx, w) in enumerate(vocabulary)}\n",
        "idx2word = {idx: w for (idx, w) in enumerate(vocabulary)}\n",
        "\n",
        "vocabulary_size = len(vocabulary)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Io55m5LP5zn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "66fe7110-3b6c-4249-dd72-72296990c8dc"
      },
      "source": [
        "idx2word"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 'he',\n",
              " 1: 'is',\n",
              " 2: 'a',\n",
              " 3: 'king',\n",
              " 4: 'she',\n",
              " 5: 'queen',\n",
              " 6: 'man',\n",
              " 7: 'woman',\n",
              " 8: 'warsaw',\n",
              " 9: 'poland',\n",
              " 10: 'capital',\n",
              " 11: 'berlin',\n",
              " 12: 'germany',\n",
              " 13: 'paris',\n",
              " 14: 'france'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzW1SWO2PUl_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "window_size = 2\n",
        "idx_pairs = []\n",
        "# for each sentence\n",
        "for sentence in tokenized_corpus:\n",
        "    indices = [word2idx[word] for word in sentence]\n",
        "    # for each word, threated as center word\n",
        "    for center_word_pos in range(len(indices)):\n",
        "        # for each window position\n",
        "        for w in range(-window_size, window_size + 1):\n",
        "            context_word_pos = center_word_pos + w\n",
        "            # make soure not jump out sentence\n",
        "            if context_word_pos < 0 or context_word_pos >= len(indices) or center_word_pos == context_word_pos:\n",
        "                continue\n",
        "            context_word_idx = indices[context_word_pos]\n",
        "            idx_pairs.append((indices[center_word_pos], context_word_idx))\n",
        "\n",
        "idx_pairs = np.array(idx_pairs) # it will be useful to have this as numpy array"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IB7YtwIOPdYJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1257
        },
        "outputId": "f1d63352-a273-4a8a-b063-17748be26245"
      },
      "source": [
        "idx_pairs"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0,  1],\n",
              "       [ 0,  2],\n",
              "       [ 1,  0],\n",
              "       [ 1,  2],\n",
              "       [ 1,  3],\n",
              "       [ 2,  0],\n",
              "       [ 2,  1],\n",
              "       [ 2,  3],\n",
              "       [ 3,  1],\n",
              "       [ 3,  2],\n",
              "       [ 4,  1],\n",
              "       [ 4,  2],\n",
              "       [ 1,  4],\n",
              "       [ 1,  2],\n",
              "       [ 1,  5],\n",
              "       [ 2,  4],\n",
              "       [ 2,  1],\n",
              "       [ 2,  5],\n",
              "       [ 5,  1],\n",
              "       [ 5,  2],\n",
              "       [ 0,  1],\n",
              "       [ 0,  2],\n",
              "       [ 1,  0],\n",
              "       [ 1,  2],\n",
              "       [ 1,  6],\n",
              "       [ 2,  0],\n",
              "       [ 2,  1],\n",
              "       [ 2,  6],\n",
              "       [ 6,  1],\n",
              "       [ 6,  2],\n",
              "       [ 4,  1],\n",
              "       [ 4,  2],\n",
              "       [ 1,  4],\n",
              "       [ 1,  2],\n",
              "       [ 1,  7],\n",
              "       [ 2,  4],\n",
              "       [ 2,  1],\n",
              "       [ 2,  7],\n",
              "       [ 7,  1],\n",
              "       [ 7,  2],\n",
              "       [ 8,  1],\n",
              "       [ 8,  9],\n",
              "       [ 1,  8],\n",
              "       [ 1,  9],\n",
              "       [ 1, 10],\n",
              "       [ 9,  8],\n",
              "       [ 9,  1],\n",
              "       [ 9, 10],\n",
              "       [10,  1],\n",
              "       [10,  9],\n",
              "       [11,  1],\n",
              "       [11, 12],\n",
              "       [ 1, 11],\n",
              "       [ 1, 12],\n",
              "       [ 1, 10],\n",
              "       [12, 11],\n",
              "       [12,  1],\n",
              "       [12, 10],\n",
              "       [10,  1],\n",
              "       [10, 12],\n",
              "       [13,  1],\n",
              "       [13, 14],\n",
              "       [ 1, 13],\n",
              "       [ 1, 14],\n",
              "       [ 1, 10],\n",
              "       [14, 13],\n",
              "       [14,  1],\n",
              "       [14, 10],\n",
              "       [10,  1],\n",
              "       [10, 14]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O9syz6ZdS4IQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_input_layer(word_idx):\n",
        "    x = torch.zeros(vocabulary_size, dtype=torch.float)\n",
        "    x[word_idx] = 1.0\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wrKkwuRP4E8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1806
        },
        "outputId": "8cfda94a-265e-4b24-cea3-c368fc8637e4"
      },
      "source": [
        "embedding_dims = 5\n",
        "# W1 = Variable(torch.randn(embedding_dims, vocabulary_size).float(), requires_grad=True)\n",
        "W1 = torch.rand(embedding_dims, vocabulary_size, requires_grad=True)\n",
        "# W1.requires_grad = True\n",
        "# W2 = Variable(torch.randn(vocabulary_size, embedding_dims).float(), requires_grad=True)\n",
        "W2 = torch.rand(vocabulary_size, embedding_dims, requires_grad=True)\n",
        "# W2.requires_grad = True\n",
        "num_epochs = 1001\n",
        "learning_rate = 0.001\n",
        "\n",
        "for epo in range(num_epochs):\n",
        "    loss_val = 0\n",
        "    for data, target in idx_pairs:\n",
        "        x = get_input_layer(data)\n",
        "        y_true = torch.tensor(np.array([target]), dtype=torch.long)\n",
        "        z1 = torch.matmul(W1, x)\n",
        "        z2 = torch.matmul(W2, z1)\n",
        "        loss = F.cross_entropy(z2.view(1,-1), y_true)\n",
        "        loss_val += loss.item()\n",
        "        loss.backward()\n",
        "        W1.data -= learning_rate * W1.grad\n",
        "        W2.data -= learning_rate * W2.grad\n",
        "        W1.grad.data.zero_()\n",
        "        W2.grad.data.zero_()\n",
        "    if epo % 10 == 0:    \n",
        "        print(f'Loss at epo {epo}: {loss_val/len(idx_pairs)}')"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss at epo 0: 2.741964670589992\n",
            "Loss at epo 10: 2.680916748728071\n",
            "Loss at epo 20: 2.626224524634225\n",
            "Loss at epo 30: 2.577154949733189\n",
            "Loss at epo 40: 2.533135373251779\n",
            "Loss at epo 50: 2.493671294621059\n",
            "Loss at epo 60: 2.4582931143896922\n",
            "Loss at epo 70: 2.426531822340829\n",
            "Loss at epo 80: 2.3979229296956746\n",
            "Loss at epo 90: 2.3720299397196087\n",
            "Loss at epo 100: 2.3484697716576712\n",
            "Loss at epo 110: 2.3269288676125663\n",
            "Loss at epo 120: 2.3071653519357955\n",
            "Loss at epo 130: 2.2889973878860475\n",
            "Loss at epo 140: 2.272287210396358\n",
            "Loss at epo 150: 2.2569222586495536\n",
            "Loss at epo 160: 2.2428015266145978\n",
            "Loss at epo 170: 2.229827632222857\n",
            "Loss at epo 180: 2.2179020404815675\n",
            "Loss at epo 190: 2.206925548825945\n",
            "Loss at epo 200: 2.19679993220738\n",
            "Loss at epo 210: 2.1874301842280794\n",
            "Loss at epo 220: 2.1787268570491247\n",
            "Loss at epo 230: 2.1706077780042374\n",
            "Loss at epo 240: 2.1629985468728203\n",
            "Loss at epo 250: 2.1558328356061662\n",
            "Loss at epo 260: 2.1490527050835744\n",
            "Loss at epo 270: 2.142607113293239\n",
            "Loss at epo 280: 2.1364520277295793\n",
            "Loss at epo 290: 2.130549120903015\n",
            "Loss at epo 300: 2.124865344592503\n",
            "Loss at epo 310: 2.119372272491455\n",
            "Loss at epo 320: 2.1140445470809937\n",
            "Loss at epo 330: 2.108860618727548\n",
            "Loss at epo 340: 2.103801829474313\n",
            "Loss at epo 350: 2.0988509893417358\n",
            "Loss at epo 360: 2.09399334362575\n",
            "Loss at epo 370: 2.089215775898525\n",
            "Loss at epo 380: 2.084506160872323\n",
            "Loss at epo 390: 2.079853776523045\n",
            "Loss at epo 400: 2.075248735291617\n",
            "Loss at epo 410: 2.070681813785008\n",
            "Loss at epo 420: 2.0661446707589284\n",
            "Loss at epo 430: 2.0616292885371617\n",
            "Loss at epo 440: 2.057128599711827\n",
            "Loss at epo 450: 2.0526355777468\n",
            "Loss at epo 460: 2.0481434992381504\n",
            "Loss at epo 470: 2.043646226610456\n",
            "Loss at epo 480: 2.039137956074306\n",
            "Loss at epo 490: 2.0346131597246444\n",
            "Loss at epo 500: 2.0300665548869543\n",
            "Loss at epo 510: 2.025492967878069\n",
            "Loss at epo 520: 2.0208880015781947\n",
            "Loss at epo 530: 2.016247183935983\n",
            "Loss at epo 540: 2.011566230228969\n",
            "Loss at epo 550: 2.00684163911002\n",
            "Loss at epo 560: 2.0020700556891304\n",
            "Loss at epo 570: 1.9972484622682845\n",
            "Loss at epo 580: 1.9923742771148683\n",
            "Loss at epo 590: 1.9874453953334263\n",
            "Loss at epo 600: 1.982460263797215\n",
            "Loss at epo 610: 1.9774176870073592\n",
            "Loss at epo 620: 1.972317021233695\n",
            "Loss at epo 630: 1.967158431666238\n",
            "Loss at epo 640: 1.9619424445288522\n",
            "Loss at epo 650: 1.9566701786858696\n",
            "Loss at epo 660: 1.9513435925756182\n",
            "Loss at epo 670: 1.945965153830392\n",
            "Loss at epo 680: 1.940537919316973\n",
            "Loss at epo 690: 1.9350660153797694\n",
            "Loss at epo 700: 1.9295535121645246\n",
            "Loss at epo 710: 1.9240057178906032\n",
            "Loss at epo 720: 1.9184284499713353\n",
            "Loss at epo 730: 1.9128276245934623\n",
            "Loss at epo 740: 1.9072101729256765\n",
            "Loss at epo 750: 1.9015829886708941\n",
            "Loss at epo 760: 1.8959536484309605\n",
            "Loss at epo 770: 1.8903297356196813\n",
            "Loss at epo 780: 1.884718758719308\n",
            "Loss at epo 790: 1.8791286315236773\n",
            "Loss at epo 800: 1.8735671298844474\n",
            "Loss at epo 810: 1.8680417452539717\n",
            "Loss at epo 820: 1.8625595586640493\n",
            "Loss at epo 830: 1.857127399103982\n",
            "Loss at epo 840: 1.8517518656594412\n",
            "Loss at epo 850: 1.8464389511517116\n",
            "Loss at epo 860: 1.8411940063749042\n",
            "Loss at epo 870: 1.8360219665936062\n",
            "Loss at epo 880: 1.830927186352866\n",
            "Loss at epo 890: 1.8259135246276856\n",
            "Loss at epo 900: 1.820983942917415\n",
            "Loss at epo 910: 1.81614100422178\n",
            "Loss at epo 920: 1.8113869837352208\n",
            "Loss at epo 930: 1.8067232029778617\n",
            "Loss at epo 940: 1.802151106085096\n",
            "Loss at epo 950: 1.797671125616346\n",
            "Loss at epo 960: 1.7932837997164046\n",
            "Loss at epo 970: 1.7889890824045453\n",
            "Loss at epo 980: 1.7847866637366159\n",
            "Loss at epo 990: 1.7806758659226554\n",
            "Loss at epo 1000: 1.7766559856278556\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QozhI246S2vM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}