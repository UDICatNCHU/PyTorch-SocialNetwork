{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PyTorch_Logistic_Regression.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/UDICatNCHU/PyTorch-SocialNetwork/blob/master/PyTorch_Logistic_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgvzHH1mrHO2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "cancer_data = load_breast_cancer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJp0ztO97tys",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = cancer_data.data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YGd5L5418IrK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y = cancer_data.target"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qO_dF4i_WJUA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_dim, output_dim = 30, 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Olddx3t2tzyy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data\n",
        "\n",
        "class LogisticRegressionModel(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(LogisticRegressionModel, self).__init__()\n",
        "        self.linear = nn.Linear(input_dim, output_dim)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        out = self.linear(x)\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bXExUCS2uEXX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = LogisticRegressionModel(input_dim, output_dim)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()  \n",
        "learning_rate = 0.0001\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSg7ZWaTua5K",
        "colab_type": "code",
        "outputId": "9fdc0019-dc15-4dc7-99b3-3b08e27a685b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1751
        }
      },
      "source": [
        "num_epochs = 100\n",
        "iter = 0\n",
        "for epoch in range(num_epochs):\n",
        "        training_data = torch.tensor(x, dtype=torch.float)\n",
        "        labels = torch.tensor(y, dtype = torch.long)\n",
        "        \n",
        "        # Clear gradients w.r.t. parameters\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Forward pass to get output/logits\n",
        "        outputs = model(training_data)\n",
        "        \n",
        "        # Calculate Loss: softmax --> cross entropy loss\n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        # Getting gradients w.r.t. parameters\n",
        "        loss.backward()\n",
        "        \n",
        "        # Updating parameters\n",
        "        optimizer.step()\n",
        "        print('Loss: {}'.format(loss.item()))    "
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss: 7.027022838592529\n",
            "Loss: 7.003715991973877\n",
            "Loss: 6.980420112609863\n",
            "Loss: 6.95710563659668\n",
            "Loss: 6.933804512023926\n",
            "Loss: 6.9105024337768555\n",
            "Loss: 6.887192726135254\n",
            "Loss: 6.863887310028076\n",
            "Loss: 6.840588092803955\n",
            "Loss: 6.817285537719727\n",
            "Loss: 6.793979167938232\n",
            "Loss: 6.770674705505371\n",
            "Loss: 6.747372150421143\n",
            "Loss: 6.724069118499756\n",
            "Loss: 6.700771808624268\n",
            "Loss: 6.677472114562988\n",
            "Loss: 6.654177665710449\n",
            "Loss: 6.630880355834961\n",
            "Loss: 6.607583522796631\n",
            "Loss: 6.58428955078125\n",
            "Loss: 6.561000347137451\n",
            "Loss: 6.5377068519592285\n",
            "Loss: 6.51441764831543\n",
            "Loss: 6.491133213043213\n",
            "Loss: 6.467850685119629\n",
            "Loss: 6.444561004638672\n",
            "Loss: 6.421274662017822\n",
            "Loss: 6.397998809814453\n",
            "Loss: 6.374715328216553\n",
            "Loss: 6.351442813873291\n",
            "Loss: 6.328174591064453\n",
            "Loss: 6.304900646209717\n",
            "Loss: 6.28162956237793\n",
            "Loss: 6.258366107940674\n",
            "Loss: 6.235098361968994\n",
            "Loss: 6.21183443069458\n",
            "Loss: 6.188577175140381\n",
            "Loss: 6.165328025817871\n",
            "Loss: 6.142074108123779\n",
            "Loss: 6.118824481964111\n",
            "Loss: 6.095577716827393\n",
            "Loss: 6.072328567504883\n",
            "Loss: 6.049090385437012\n",
            "Loss: 6.02585506439209\n",
            "Loss: 6.002621173858643\n",
            "Loss: 5.979390621185303\n",
            "Loss: 5.956168174743652\n",
            "Loss: 5.932946681976318\n",
            "Loss: 5.909729957580566\n",
            "Loss: 5.886524200439453\n",
            "Loss: 5.863312244415283\n",
            "Loss: 5.84011173248291\n",
            "Loss: 5.816914081573486\n",
            "Loss: 5.793720722198486\n",
            "Loss: 5.770537853240967\n",
            "Loss: 5.7473530769348145\n",
            "Loss: 5.724177360534668\n",
            "Loss: 5.701007843017578\n",
            "Loss: 5.677840232849121\n",
            "Loss: 5.654682159423828\n",
            "Loss: 5.631529808044434\n",
            "Loss: 5.608387470245361\n",
            "Loss: 5.585249900817871\n",
            "Loss: 5.562116622924805\n",
            "Loss: 5.538994789123535\n",
            "Loss: 5.515876293182373\n",
            "Loss: 5.492772102355957\n",
            "Loss: 5.469676494598389\n",
            "Loss: 5.446584224700928\n",
            "Loss: 5.4235029220581055\n",
            "Loss: 5.400427341461182\n",
            "Loss: 5.3773627281188965\n",
            "Loss: 5.354304313659668\n",
            "Loss: 5.33126163482666\n",
            "Loss: 5.308222770690918\n",
            "Loss: 5.285195827484131\n",
            "Loss: 5.262186050415039\n",
            "Loss: 5.239182472229004\n",
            "Loss: 5.216188430786133\n",
            "Loss: 5.193211078643799\n",
            "Loss: 5.170244216918945\n",
            "Loss: 5.147290229797363\n",
            "Loss: 5.124358654022217\n",
            "Loss: 5.101439476013184\n",
            "Loss: 5.078535079956055\n",
            "Loss: 5.055655479431152\n",
            "Loss: 5.032803058624268\n",
            "Loss: 5.009973526000977\n",
            "Loss: 4.987170219421387\n",
            "Loss: 4.964407920837402\n",
            "Loss: 4.941688537597656\n",
            "Loss: 4.919023036956787\n",
            "Loss: 4.89639949798584\n",
            "Loss: 4.873840808868408\n",
            "Loss: 4.851352214813232\n",
            "Loss: 4.828927040100098\n",
            "Loss: 4.806567192077637\n",
            "Loss: 4.78428316116333\n",
            "Loss: 4.762065887451172\n",
            "Loss: 4.739911079406738\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_jS8oHgxUXs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}